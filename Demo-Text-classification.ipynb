{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "\n",
    "#If you have any confusion matrix then see this link:-\n",
    "# https://www.youtube.com/watch?v=0kPRaYSgblM&fbclid=IwAR0mhZAzD1IindpFnTUfLV69MwLXBbwhls8q7st_1jYws-7Qtyjm2FuXYpU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1\n",
    "# Word Count - CountVectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets gets back to our Naive Bayes Classifier what we basically doing is that we will actually count the number of times a particular word is coming in that partcular category.\n",
    "\n",
    "Then we find the probability and then we classify using Naive Bayes.\n",
    "\n",
    "But how you will make a machine to actually count the number of words.\n",
    "\n",
    "So what it basically it does is (for example  let's assume you have a big sentence) that it will give a particular unique number to each of unique words then after that it will count that how many times that a particular is coming it is not going to actually check that text but instead it will give a number to that particular text then it will count how that number is repeating.\n",
    "\n",
    "So, that giving a unique value to each and every word that is done by fit() method. It is under count-vectorizer class. \n",
    "we can do this collectively by using the method fit_transform()\n",
    "\n",
    "And then counting the number of times that particular number is comming is done by transform method\n",
    "\n",
    "                (you can see my .pdf for more info...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print Vocabulary : {'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jump': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['brown', 'dog', 'fox', 'jump', 'lazy', 'over', 'quick', 'the']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature name : ['brown', 'dog', 'fox', 'jump', 'lazy', 'over', 'quick', 'the']\n",
      "The shape of count  : (3, 8)\n",
      "Printing counts : [[1 1 1 1 1 1 1 2]\n",
      " [0 1 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Demo code to understand CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# text = [\"I am manjeet raj lakshman\",\n",
    "#        \"My college name is Army Institute of Technology\",\n",
    "#        \"Demo code to understand CountVectorizer\"]\n",
    "text = [\"The quick brown fox jump over the lazy dog\",\n",
    "       \"The dog\",\n",
    "       \"The fox\"]\n",
    "vector = CountVectorizer()\n",
    "vector.fit(text) # Learn a vocabulary dictionary of all\n",
    "                 # tokens in the row documents.\n",
    "    \n",
    "print(\"print Vocabulary : \" +str(vector.vocabulary_))\n",
    "\n",
    "vector.get_feature_names()\n",
    "print(\"feature name : \" +str(vector.get_feature_names()))\n",
    "\n",
    "counts = vector.transform(text)\n",
    "print(\"The shape of count  : \" +str(counts.shape)) #in it's shape 3 is denoting the columnof the dict(or number of sample)\n",
    "#and 17 is denoting the number of column i.e number of feature\n",
    "\n",
    "print(\"Printing counts : \"+str(counts.toarray()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2\n",
    "# Term Frequency Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous (step2/section1) we have counted the number of\n",
    "words but it has one issue.\n",
    "\n",
    "what is the issue?\n",
    "-for example the word like 'though' it will come as so\n",
    "many times but it's large count doesn't make any sense in\n",
    "the classification of the documents.\n",
    "\n",
    "So what the 'term frequency' and 'Inverse Document' does is?\n",
    "-Term Frequency :- it means number of times that \n",
    "                   particular word is coming\n",
    "\n",
    "                   \n",
    "-Inverse Document :- it will actually give the weight\n",
    "                     that how much it is important for \n",
    "                     the classification.\n",
    "                     \n",
    "So the word like 'though will get the minimum weight\n",
    "for the classification\n",
    "\n",
    "This can be done by using one of the class of scikit-learn that is = sklearn.feature_extraction.text.TfidfVectorizer\n",
    "in that we will use the method fit_transform()\n",
    "\n",
    "                (you can see in .pdf file for info...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning frequency of all features : [1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n",
      "Transforming the matrix based on the learnt frequencies or weight : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.36388646, 0.27674503, 0.27674503, 0.36388646, 0.36388646,\n",
       "        0.36388646, 0.36388646, 0.42983441],\n",
       "       [0.        , 0.78980693, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.61335554],\n",
       "       [0.        , 0.        , 0.78980693, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.61335554]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo code to understand TfidfTransformer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# create the transform method  \n",
    "vectorizer = TfidfTransformer()\n",
    "\n",
    "vectorizer.fit(counts) #Learning frequency of all features\n",
    "print(\"Learning frequency of all features : \" +str(vectorizer.idf_))\n",
    "#you can see word like \"The\" is getting least weight\n",
    "\n",
    "freq = vectorizer.transform(counts) # Transforming the matrix \n",
    "#based on the learnt frequencies or weight of all feature(count matrix that i have learnt before)\n",
    "print(\"Transforming the matrix based on the learnt frequencies or weight : \")\n",
    "freq.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3\n",
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as skd\n",
    "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
    "news_train = skd.load_files('/home/manjeet/Videos/Documentation/ML-algo/Text-classification-using-Naive-Bayes/data/train',categories=categories,encoding='ISO-8859-1')\n",
    "news_test = skd.load_files('/home/manjeet/Videos/Documentation/ML-algo/Text-classification-using-Naive-Bayes/data/test',categories=categories,encoding='ISO-8859-1')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "x_train_tf = count_vect.fit_transform(news_train.data)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "x_train_tfidf = tfidf_transformer.fit_transform(x_train_tf)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(x_train_tfidf,news_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = ['God is love','OpenGL on the CPU is fast']\n",
    "x_new_counts = count_vect.transform(docs_new) #here i'm not using fit_transfom()\n",
    "# why, because fit() is used for learning and i have already \n",
    "# used before to learn the module now what i have to do is \n",
    "# whatever the sentences is comming for prediction i have to\n",
    "# transform it based on my learning that i have already learnt\n",
    "# before and learning is done by fit().So now only i need to\n",
    "# transform the incomming sentences. that's why i am not using\n",
    "# fit_transfom() method\n",
    "x_new_tfidf = tfidf_transformer.transform(x_new_counts) # same reason\n",
    "# why i am only using transform(),not fit().\n",
    "predicted = clf.predict(x_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted #here 3 means 'God is love' is belongs to 4th categories i.e soc.religion.christian\n",
    "# and 1 means 'OpenGL on the CPU is fast' belongs to 2nd category i.e comp.graphics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4\n",
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to deail with the test data:-\n",
    "- Take you new test data\n",
    "- Again CountVectorize it\n",
    "- Use TD IDF transformer\n",
    "- Then predict it using MultinomialNB()\n",
    "- Analyse the result\n",
    "- METRICS.Classification_report()\n",
    "  .Precision    .micro-avg\n",
    "  .Recall       .macro-avg\n",
    "  .F1-score     .weighted-avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1502, 35788)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_tf = count_vect.transform(news_test.data)\n",
    "x_test_tf.shape\n",
    "x_test_tfidf = tfidf_transformer.transform(x_test_tf)\n",
    "\n",
    "predicted = clf.predict(x_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8348868175765646\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.97      0.60      0.74       319\n",
      "         comp.graphics       0.96      0.89      0.92       389\n",
      "               sci.med       0.97      0.81      0.88       396\n",
      "soc.religion.christian       0.65      0.99      0.78       398\n",
      "\n",
      "             micro avg       0.83      0.83      0.83      1502\n",
      "             macro avg       0.89      0.82      0.83      1502\n",
      "          weighted avg       0.88      0.83      0.84      1502\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[192,   2,   6, 119],\n",
       "       [  2, 347,   4,  36],\n",
       "       [  2,  11, 322,  61],\n",
       "       [  2,   2,   1, 393]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy : \", accuracy_score(news_test.target,predicted))\n",
    "print(metrics.classification_report(news_test.target,predicted,target_names=news_test.target_names))\n",
    "\n",
    "metrics.confusion_matrix(news_test.target,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
